{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EPFL Machine Learning Higgs\n",
    "\n",
    "## Loading and preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from helpers import load_data, one_hot_encode, standardize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "data_directory = '../data'\n",
    "train_dataset_path = os.path.join(data_directory, 'train.csv')\n",
    "public_test_dataset_path = os.path.join(data_directory, 'test.csv')\n",
    "\n",
    "# Loading the data\n",
    "_, Y_train_public, feature_names, X_train_public = load_data(train_dataset_path)\n",
    "ids_test_public, _, _, X_test_public = load_data(public_test_dataset_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of missing values: 0.0\n",
      "Proportion of missing values: 0.0\n"
     ]
    }
   ],
   "source": [
    "# We need to deal with -999 somehow (missing values)\n",
    "# For now just assume a naive approach and set the values to 0\n",
    "# Since they are floats, we add an epsilon against numerical errors\n",
    "EPSILON = 1E-4\n",
    "mask_train = np.abs(X_train_public + 999) <= EPSILON\n",
    "mask_test = np.abs(X_test_public + 999) <= EPSILON\n",
    "print('Proportion of missing values:', np.sum(mask_train)/(mask_train.shape[0]*mask_train.shape[1]))\n",
    "print('Proportion of missing values:', np.sum(mask_test)/(mask_test.shape[0]*mask_test.shape[1]))\n",
    "\n",
    "X_train_public[mask_train] = 0\n",
    "X_test_public[mask_test] = 0\n",
    "\n",
    "# We will standardize the data based on the mean and standard deviation of the !!! public train dataset !!! (is this okay? We are basically estimating the population mean and std tis way)\n",
    "# We can also try normalizing between 0-1, since some values stay quite large\n",
    "# ! The method standardizes in-place !\n",
    "continuous_column_idxs = np.where(feature_names != \"PRI_jet_num\")[0]\n",
    "column_means, column_stds = standardize(X_train_public, continuous_column_idxs)\n",
    "_, _ = standardize(X_test_public, continuous_column_idxs, column_means, column_stds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# We will need to add interaction terms to deal with co-linearity somewhere (like x1*x2 instead of just x1 and x2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Note to self: Make sure to standardize before one-hot encoding, otherwise specifying the columns could be annoying\n",
    "\n",
    "# Then we will need to notice the discrete-valued column, since this needs to be one-hot encoded\n",
    "# In our dataset, only \"PRI_jet_num\" is discrete.\n",
    "discrete_column_idxs = np.where(feature_names == \"PRI_jet_num\")[0]\n",
    "\n",
    "# Update the features by one-hot encoding the discrete ones, but only update the feature names at the end\n",
    "# They will be the same for the train and test set anyway\n",
    "X_train_public, _ = one_hot_encode(X_train_public, discrete_column_idxs, feature_names)\n",
    "X_test_public, feature_names = one_hot_encode(X_test_public, discrete_column_idxs, feature_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 164333\n",
      "Number of negative samples: 85667\n"
     ]
    }
   ],
   "source": [
    "# Since this is a binary classification problem, we do not need to one-hot encode the y-vector, but we can just use binary values\n",
    "positive_sample = 'b'\n",
    "negative_sample = 's'\n",
    "Y_train_public = np.expand_dims((Y_train_public == positive_sample).astype(np.int32), axis=1)\n",
    "\n",
    "# ! There are quite some more positive than negative samples, maybe we could try to weigh negative samples more or something? !\n",
    "print('Number of positive samples:', np.sum(Y_train_public))\n",
    "print('Number of negative samples:', len(Y_train_public) - np.sum(Y_train_public))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and analysis\n",
    "\n",
    "After preprocessing, the new features of the public train- and test dataset are stored in \"X_train_public\" and \"X_test_public\". The labels are binary values stored in \"Y_train_public\". Furthermore, the feature column names are found in \"feature_names\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DER_mass_MMC' 'DER_mass_transverse_met_lep' 'DER_mass_vis' 'DER_pt_h'\n",
      " 'DER_deltaeta_jet_jet' 'DER_mass_jet_jet' 'DER_prodeta_jet_jet'\n",
      " 'DER_deltar_tau_lep' 'DER_pt_tot' 'DER_sum_pt' 'DER_pt_ratio_lep_tau'\n",
      " 'DER_met_phi_centrality' 'DER_lep_eta_centrality' 'PRI_tau_pt'\n",
      " 'PRI_tau_eta' 'PRI_tau_phi' 'PRI_lep_pt' 'PRI_lep_eta' 'PRI_lep_phi'\n",
      " 'PRI_met' 'PRI_met_phi' 'PRI_met_sumet' 'PRI_jet_leading_pt'\n",
      " 'PRI_jet_leading_eta' 'PRI_jet_leading_phi' 'PRI_jet_subleading_pt'\n",
      " 'PRI_jet_subleading_eta' 'PRI_jet_subleading_phi' 'PRI_jet_all_pt'\n",
      " 'PRI_jet_num_0' 'PRI_jet_num_1' 'PRI_jet_num_2' 'PRI_jet_num_3']\n",
      "(250000, 33) (568238, 33)\n",
      "(250000, 1)\n"
     ]
    }
   ],
   "source": [
    "# We will do k-fold cross validation to create subsets of the training and testing datasets\n",
    "print(feature_names)\n",
    "print(X_train_public.shape, X_test_public.shape)\n",
    "print(Y_train_public.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:09<00:00, 50.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD loss: 0.08500413180707353\n",
      "\"Optimal\" loss: [[0.08444842]]\n"
     ]
    }
   ],
   "source": [
    "# Now we can do some training\n",
    "from implementations import least_squares_GD, ridge_regression\n",
    "\n",
    "w, loss = least_squares_GD(Y_train_public, X_train_public, np.zeros(shape=(X_train_public.shape[1], 1)), 500, 0.1)\n",
    "\n",
    "w_optim, loss_optim = ridge_regression(Y_train_public, X_train_public, lambda_=0)\n",
    "\n",
    "print('GD loss:', loss)\n",
    "print('\"Optimal\" loss:', loss_optim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we can evaluate the training\n",
    "\n",
    "# ! We can actually also do some hyperparameter tuning (using ROC curves to determine the right cut-off probability) !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do predictions on the public test dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# Placeholder, array with True of False whether it belongs to the positive or negative sample\n",
    "predictions = np.random.randint(2, size=ids_test_public.shape).astype(bool)\n",
    "\n",
    "submission_file_name = 'submission_0.csv'\n",
    "with open(os.path.join(data_directory, submission_file_name), mode='w', newline='', encoding='utf-8') as submission_file:\n",
    "    writer = csv.writer(submission_file, delimiter=',')\n",
    "    writer.writerow(['Id', 'Prediction'])\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        writer.writerow([ids_test_public[i], positive_sample if prediction else negative_sample])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
